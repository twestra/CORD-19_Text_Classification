The recent increase of immunopeptidomic data, obtained by mass spectrometry (MS) or binding assays, opens unprecedented possibilities for investigating endogenous antigen presentation by the highly polymorphic human leukocyte antigen class I (HLA-I) protein. We introduce a flexible and easily interpretable peptide presentation prediction method. We validate its performance as a predictor of cancer neoantigens and viral epitopes and use it to reconstruct peptide motifs presented on specific HLA-I molecules.
Recognition of malignant and infected cells by the adaptive immune system requires binding of cytotoxic T-cell receptors to antigens, 8-11-mer peptides presented by the Major Histocompatibilty Complex (MHC) class I coded by HLA-I alleles (Fig. 1a) . Tumour-specific neoantigens are currently sought-after targets for improving cancer immunotherapy [1, 2]. Computational predictions can help select potential neoantigens and accelerate immunogenicity testing. To be useful, these predictions must be specific to each HLA type.
State-of-the-art methods [3][4][5], such as NetMHC [6, 7], are based on artificial neural networks trained in a supervised way to predict peptide presentation from known peptide-HLA association. They must be trained on large datasets, and perform best on common alleles. Their accuracy is degraded for rare or little studied HLA-I alleles which are poorly represented in databases. In that case, another approach is to train unsupervised models of presentation from custom elution experiments with little or no information about peptide-HLA association. For instance, MixMHCp [8, 9] can reconstruct, from unannotated peptide sequences, a mixture of generative models -one for each expressed HLA type. However, it makes simplifying assumptions about binding specificity, and is not designed to leverage available (albeit limited) annotation information from the Immune Epitope Database [10] (IEDB) to improve accuracy.
We present an alternative method for predicting peptides presented by specific HLA types, which can be trained on custom datasets. It can be applied to patient-or experiment-specific samples and treats common and rare alleles on equal footing, avoiding potential database biases. We use a Restricted Boltzmann Machine (RBM), an unsupervised machine learning scheme that learns probability distributions of sequences given as input [11] [12] [13] . The RBM estimates presentation scores for each peptide, and can generate candidate presentable peptides. It also provides a lower dimensional representation of peptides with a clear interpretation in terms of associated HLA type, which can be exploited to train classifiers of HLA association from a small number of HLA annotations. The RBM has a simple structure with one hidden layer and weights connecting the input -the peptide sequence -to the hidden layer. The weights, along with the biases acting on both input and hidden units, are learned from a list of presented peptides (see Methods, Fig. 1b, Supplementary Figs. 1, 2, 3) . The RBM probability is interpreted as a probabilistic score of antigen presentation by any of the 6 HLA-I in the experiment. We applied standard alignment routines to reduce sequences to the most common length of 9 residues (see Methods).
As a first validity check, we built an RBM model to predict presentation by a single common allele, HLA-A*02:01, from MS data annotated with this HLA restriction in IEDB. We tested the model's ability to discriminate presented peptides from "generic" peptides (randomly drawn from the human proteome). We calculated a Receiver Operating Characteristic (ROC) curve from RBM presentation scores assigned to a test set of presented versus generic peptides (Fig. 1c) . The Area Under the Curve of the receiving operating characteristic, AUC = 0.973, proves the RBM predictive power at recovering presented antigens. To further probe the applications of RBM to neoantigen discovery, we considered the missense mutations in 5 ovarian and melanoma cancer cell lines, filtered by expression levels and by the NetMHC-predicted affinity to HLA*A:02:01 [14] . We attributed RBM presentation scores to all 8-11-mer peptides harbouring these mutations. RBM performed comparably to , the state-of-the-art method for antigen prediction: 15 out of 16 neoantigens that were MS-validated as HLA-I bound are ranked by the RBM in the (which was not certified by peer review)
top 4% among generic peptides (mean percentile 1.4% versus 1.7% for NetMHC), and the top 10% among the mutated peptides (mean 4.2% versus 2.5% for NetMHC) considered for the corresponding cell line, Fig.  1c -d and Methods.
Next, we applied the RBM to complex data on which it can show its efficiency: multi-allelic eluted samples where peptides may be bound to up to 6 different HLA-I proteins. The goal is to reconstruct binding specificity motifs, i.e. classify epitopes by HLA preference. To test performance in a case where the ground truth is known, we trained RBMs on 10 synthetic "single-individual" samples pooling IEDB antigens presented by 6 HLA-I proteins per individual, covering 43 different alleles in total (12 HLA-A, 17 HLA-B, 14 HLA-C, see Methods). Each HLA-I recognizes specific sequence "motifs" (preferred residues at each position) in the presented peptides. RBM hidden units allow for the mapping of peptide sequences onto a lower dimensional "representation space" (see Methods) which organizes them into clusters by motif, reflecting different HLA-binding specificities present in the sample (Fig. 1b) . The data-derived RBM parameters underlying that representation play a key role in capturing what amino acids contribute to these binding motifs ( Supplementary Fig. 4) .
The low-dimensional representation of peptides (Fig. 1b) suggests an efficient way to classify them, using the annotation of a small number of sequences with their HLA preference. In practice these annotated sequences may come from other experiments or public databases. In our synthetic example, we randomly selected 10% of peptides and labeled them with their associated HLA-I. Using these annotations, we trained a linear classifier to predict the HLA-I restriction of individual peptides from their RBM representation (see Methods, Fig. 1b, Supplementary Figs. 1, 5) . We refer to this architecture, which combines the RBM (trained on unannotated peptides) and the HLA-I type classifier (trained on a few annotated ones), as RBM-MHC. The performance of RBM-MHC at predicting HLA association, as measured by AUC = 0.993, is excellent (see Fig. 1e and Methods). The RBM representation is crucial for achieving high prediction performance. Training a linear HLA-I classifier directly on the annotated sequences (rather than their RBM representation) yields a much poorer performance (AUC=0.724). On the other hand, a completely unsupervised clustering algorithm (K-means [15] ) applied to unlabeled data in RBM representation space ('RBM-km') performed well (AUC=0.935), while K-means applied directly to sequences did not (AUC=0.645, Fig. 1e and Methods).
RBM-MHC outperforms MixMHCp [8, 9] in terms of both accuracy and stability across the 10 synthetic datasets, in part thanks to the 10% labeled data it exploits. The RBM also captures global sequence correlations that MixMHCp's independent-site models miss, which are important for correctly classifying antigens across alleles with similar binding motifs. The major drop in MixMHCp performance occurs precisely in datasets that mix same-supertype alleles [16] (HLA-B*27:05 and HLA-B*39:01 belonging to supertype B27, HLA-B*40:01 and HLA-B*44:03 belonging to B44, see Supplementary Table 1) . RBM-MHC also performs better than NetMHCpan-4.0 [7] , which is trained on a separate, IEDB-derived dataset to predict binding to each of the 6 alleles in our datasets. The gain in performance is slightly more pronounced for 9-mers-only samples than for 7) , suggesting room for improvement in accounting for variable lengths. Furthermore, the structure and interpretation of the RBM representation space allows for the generation of new, artificial peptides from the model with controled HLA-binding specificity (see Supplementary Note, Supplementary Fig. 14) .
To assess the relevance of our approach in a clinical setting, we considered single-patient, melanomaassociated immunopeptidomic datasets [17] [18] [19] , complemented with patient HLA typing and Whole Exome Sequencing (WES) of tumorous cells, where in total 11 neoantigens were identified. We tested the the RBM-MHC approach for motif reconstruction. Since in this case true peptide-HLA associations are not known, model performance is evaluated through the correlation between the predicted motifs and motifs reconstructed from IEDB monoallelic data ( Fig. 2a-b, Supplementary Figs . 8, 9, 10) . RBM-MHC, MixMHCp and NetMHCpan4.0 perform comparably, with the only exception of Mel15, partially explained by the limited number of available labels (for HLA-A*68:01 and HLA-B*35:03). RBM-MHC and its unsupervised version RBM-km, when trained on the patient dataset (see Methods) systematically predicted the correct HLA association for all neoantigens. They also assigned a top 1% score among all WES mutants to 8 out of 11 identified neoantigens (Supplementary Table 2 ).
Beyond their usefulness for predicting HLA specificity, RBM hidden units serve as a tool to discover additional features within the same HLA-binding specificity. A compelling case is the fast identification of subclasses in HLA-B*51:01 ligands corresponding to the two structurally alternative binding modes validated in [9] (Fig. 2c, Supplementary Fig. 4 ).
In conclusion, RBM-MHC efficiently exploits the statistical information of unannotated, moderate-size datasets of antigen presentation available from clinical studies, and combines it with small amounts of antigen annotations (as few as < 100, typical of poorly represented HLA-I alleles) to deliver accurate and stable Figure 1 : RBM-MHC approach to HLA-I antigen presentation. a, Antigens binding to HLA match specific "sequence motifs" represented by logos (example of Tax-HLA-A*02:01 complex structure, PDB-ID:1BD2, Mol* image [22] [14] . e, Performance (AUC) of 6 methods for HLA-I prediction on on 9-mers in 10 synthetic-individual samples, each carrying 6 HLA-I covering A,B,C allels (see Supplementary Table 1) . Bars are standard deviations over 10 datasets.
. predictions of HLA-I binding. Our results show that the approach is useful for systematic applications with newly produced large-scale datasets [5] . We plan to develop an extension to HLA class II presentation. As the MS amino acid frequency distribution is potentially affected by biases of the technique, we also designed a sequence re-weighting scheme to avoid overfitting and better score ligands tested in vitro (Supplementary Note, Supplementary Fig. 13 ). We stress that the method is not limited to MS datasets. As a timely application of the approach to SARS-CoV-2, we showed that affinity-trained RBM models recover with high probability SARS-CoV-2 HLA-I epitopes validated in vitro [20] ( Supplementary Fig. 11 ). MStrained RBM-MHC outperforms NetMHC versions [6, 7] used in Ref. [20] for rare HLA-C alleles (Fig. 2d) , although the validated peptides were selected by NetMHC itself. RBM-MHC also assigns high presentation scores to SARS-CoV-2 epitopes that are homologous to experimentally validated SARS-CoV cytotoxic Tcell epitopes [21] , on par with NetMHC4.0 [6] (Fig. 2e) and NetMHCpan-4.0 [7] (Supplementary Fig. 12 ). In general, our method is designed to be trained on custom samples, which could be of relevance to produce sample-specific insights about the complexity of endogenous HLA-I presentation. Figure 2 : Predictions for cancer samples and SARS-CoV-2 epitopes. a, Sequence logos of clusters found with RBM-MHC trained on melanoma-associated sample 12T [17, 18] and IEDB monoallelic data; ρ shows Pearson correlation between respective amino acid frequencies. b, Performances (average ρ over clusters) for 3 samples from [17] [18] [19] . MixMHCp merges Mel8 antigens specific to the 2 HLA-C into the same cluster, causing a drop in the average ρ (see Supplementary Fig. 9 ). c, RBM distinguishes 2 subclasses in HLA-B*51:01-binding antigens. The HLA-B*51:01-peptide bond can be established [9] : (i) via the interaction of the HLA-B*51:01 residue 62 with peptide position 2, requiring a hydrophilic residue there, typically Alanine (A), and a polar or negatively charged residue, typically Aspartic Acid (D), at position 1; (ii) with R62 sidechain facing the solvent, requiring co-occurrence at positions 1-2 of hydrophobic residues, typically Proline (P), at position 2). Inspection of the inputs to the 10th hidden unit (h 10 ), found by projecting peptides predicted by RBM-MHC as HLA-B*51:01-specific onto the corresponding weights (Methods, eq. 4), reveals a bimodal distribution, enabling the discrimination of the "hydrophilic/charged" pattern (i) from the "hydrophobic" pattern (ii), as recapitulated by the sequence logos. d, Correlation of experimental stability of SARS-CoV-2 epitopes [20] with scores from NetMHC (logK d -dissociation constant [nM]) and RBM trained on Binding Affinity (BA) and Mass Spectroscopy (MS) data. e, Score percentiles (relative to all SARS-CoV-2 9-mers peptides) of homologs of dominant SARS-CoV epitopes. Boxplots indicate median, upper and lower quartiles.
RBM-MHC algorithm. We use RBM-MHC to predict antigen sequences presented on specific HLA types. The RBM-MHC architecture ( Fig. 1b and Supplementary Fig. 1 ) is composed of two parts, which are trained in successive steps. The first, a Restricted Boltzmann Machine, is learned in an unsupervised way, while the second, a HLA-I classifier, is fully supervised but only by a minimal amount of known antigen-HLA-I associations (semi-supervised).
Restricted Boltzmann Machine (RBM). The RBM learns an underlying probability model for the antigen sequences in the training datasets, in our case the HLA-I presented antigens. A RBM [11, 12] consists of one layer of N v observed units Supplementary Fig. 1) . Mathematically, the model is defined by a joint probability over presented antigen sequences and hidden units:
where g i (v i ) are local potentials acting on observed units, U µ (h µ ) are local potential on hidden units and the weights w iµ couple hidden and observed units. The parametric form of U µ (h µ ) is chosen as a double Rectified Linear Units (dReLu):
containing parameters (γ µ,+ , γ µ,− , h + , h − ) to infer from data during training (see below). The dReLu was shown to outperform other choices of potential such as Gaussian [13, 23] , guaranteeing that correlations beyond pairwise in data are captured. The probability distribution over the presented antigen sequences one is interested in modelling is recovered as the marginal probability over hidden units:
where Γ µ I(v) = log dh e −U µ (h)+hI . We define I µ (v), the input to hidden unit µ coming from the observed sequence v, as the sum of the weights entering that particular hidden unit:
During the modelling step called "training", the weights w iµ , the local potentials g i (v i ) and the parameters specifying U µ (h µ ) are inferred from data by maximizing, through stochastic gradient ascent, the average log-likelihood L RBM = log P(v) data of sequence data v, as previously described [13, 23] . This leads to inferring the RBM probability distribution that optimally reproduces the statistics of the training dataset. During training, the contribution of sequences to L RBM can be re-weighted following a re-weigthing scheme we have conceived (see Supplementary Note) to correct for amino-acid frequency biases in the training dataset, as the ones introduced by mass spectrometry (MS) detection. A regularization, i.e. a penalty term over the weight parameters is introduced to prevent overfitting. The function maximized during training then becomes:
where q = 21 is the number of symbols appearing in observed sequences (20 amino acids and the alignment gap). This regularization, which was first introduced in [13] , plays effectively the role of a L 1 regularization, imposing sparsity of weights, with a strength that is adapted to increasing magnitude of weights, hence favouring homogeneity among hidden units (see [13] for more detailed explanations). Examples of inferred sets of weights at different regularization strength λ 2 1 are provided in Supplementary Figs. 3-4. The package used for training, evaluating and visualising RBMs is an updated version of the one described in Refs. [13, 23, 24] . The package was ported to Python3 and the optimization algorithm was changed from SGD to RMSprop (i.e. to ADAM without momentum with parameters l r = 5x10 −3 , β 1 = 0, β 2 = 0.99, ε = 10 −3 ). The adaptive learning rate of RMSprop/ADAM result in larger updates for the fields and weights attached to rare amino acids, and hence speeds up convergence.
The HLA-A*02:01-specific RBM presentation model is trained with 5 hidden units and λ 2 1 = 0.001, the multi-allelic RBM models are trained with 10 hidden units and λ 2 1 = 0.001. The (hyper-)parametric search to select optimal λ 2 1 regularization and the number of hidden units was made from the trend of the log-likelihood on a held-out validation dataset, the aim being to achieve a good fit but to avoid overfitting (Supplementary Figs. 2a-b) .
HLA-I Classifier. The HLA-I classifier part of the RBM-MHC takes as input
of the annotated peptide sequences and gives a categorical output, i.e. the HLA-I specificity of the peptide v ( Fig.  1b and Supplementary Fig. 1 ). c = 1, ..., N c denotes the HLA-I type. Typically for single-individual samples N c = 6, from the 2 HLA-A, HLA-B, HLA-C. The classifier is trained by minimizing a loss function chosen to be a categorical cross-entropy S :
where the v sum runs only over the sequences labeled with their HLA-I association. y c (v) is the label assigned to v for supervised training in one-hot encoding, i.e. y c (v) = 1 only for the c standing for its associated HLA type and zero otherwise so ∑ c y c (v) = 1 (it is normalized over categories). The choice of one-hot encoding is justified by the fact that, for the sake of discriminating motifs, we select peptides associated to only 1 HLA type in the database (mainly from monoallelic sources, see below).ŷ(v) is the categorical output predicted by the HLA-I classifier for v, calculated from the softmax activation function:
where X are the classifier weights, connecting input to output layer (see Supplementary Fig. 1 ), and b are local biases adjusted during training. Element-wise softmax is defined as:
The activation function softmax has the advantage of giving predictions normalized over categories, thus each elementŷ c (v) can be interpreted as the probability that sequence v belongs to class c. Our numerical implementation relies on Theano and Keras Python libraries. Training is performed in minibatches of 64 sequences, by the Adam optimizer for 1000 epochs, retaining the model that gives the best accuracy on a held-out partition of the ∼30% of the training dataset. The choice of RBM (hyper-)parameters (see above) also ensures a high accuracy of classification ( Supplementary Fig. 2c ). Accuracy of classification is measured there as an Area Under the Curve (AUC), which is different from the cross-entropy optimized during training (Eq. 6) and is defined below, in the section "RBM-MHC performance at motif reconstruction in multi-allelic samples". This AUC value is only minimally affected when reducing the RBM training dataset and is close to 1 (indicating the maximal accuracy) already with the small number of labeled sequences used, i.e. 10% of data. (The AUC clearly increases further when increasing this amount, see Supplementary Fig.  5 ).
Combining the probability functions estimated by RBM and HLA-I classifier, we define for each sequence v a global score L (v):
where L RBM (v) = log P(v) is the RBM log-likelihood assigned to sequence v and L Cl (v) is the classifier score, defined from the vector of predicted class probabilitiesŷ
is the negative entropy of classification, so it contributes to L (v) with higher values when the confidence with which a HLA-I class is predicted is higher.
Unsupervised clustering. K-means algorithm. Given a set of points x, K-means [15, 25] finds the centroids of c = 1, ..., N c clusters and assigns each x to the cluster whose centroid has the minimal distance to x. If we indicate by d c (x) the distance between point x and a cluster c, we can express the probability that x belongs to cluster c as:ŷ
as is a common choice in the "soft" version of K-means. With the approach we refer to as RBM-km we apply K-means to sequence representations in the space of inputs to hidden units (i.e. x = I(v)) instead of sequences themselves (i.e. x = v). From such a probabilistic clustering prediction, we define the classification score ∑ N c c=1ŷ km c (x) log(ŷ km c (x)). Our implementation of K-means relies on the routine available in the Python package Scikit-learn [26] .
MixMHCp algorithm. Consistently with the other approaches discussed, we assume that the expected number of clusters N c is known, and we implement the MixMHCp clustering [8, 9] . First we build Position Weight Matrices (PWM) for each of the N c clusters found by MixMHCp; each PWM describes the singlesite amino acid frequencies f c i (v i ) in cluster c, with c = 1, ..., N c . For a sequence v a set of presentation scores (one per cluster) can be defined from the log-likelihood under the corresponding PWM:
where the superscript MM stands for "MixMHCp" and L is the length to which all sequences are reduced by the alignment procedure in MixMHCp (L = 9). The final score of a sequence v is taken as the maximal log-likelihood among the N c clusters,
, which is used jointly to predict the HLA association.
Annotation of HLA-I binding motifs. The first step in clustering approaches, either by MixMHCp or K-means, is fully unsupervised and consists of optimally assigning peptides to N c clusters. Clusters then need to be annotated with the corresponding HLA-binding specificity, among the N c ones known to be expressed in the sample cells from e.g. HLA typing. For this second step, we consider the fraction of data for which we have labels (the same used for training the HLA-I classifier) and we estimate from them a PWM for each HLA type, in such a way as to obtain a set of reference motifs. We next estimate the PWM of each cluster and we label the cluster with the HLA association of the reference motif that minimizes the difference to the cluster PWM. Note that this mapping could give the same HLA type associated to several clusters and other HLA types not associated to any cluster, indicating a poor classification performance.
Antigen sequence alignment. To work with fixed-length sequences when learning a presentation model for datasets containing 8-11-mers, we first reduce these peptides of variable length to a homogeneous length that we set to the typical length of antigens presented by HLA class I proteins, i.e. 9 residues (the length of the antigen binding core). Our alignment routine consists of first building a reference profile of length 9 by estimating the PWM describing the subset of 9-mers in the dataset of interest. Next we align the other peptides to this profile by the insertion of a gap in 8-mers and deletions in 10-11 mers, using the functions seqprofile and profalign (with default options) of the Matlab Bioinformatics Toolbox (release R2018b). Typical samples of interest pool together peptides of different binding specificity to HLA-I, each of these being characterized by well defined sequence motifs. On one hand, the best alignment for each peptide needs to be based on the PWM of 9-mers sharing the same HLA preference, which best describes the respective sequence motif. On the other hand, identifying these subsamples sharing the same specificity requires first identifying the motif, therefore we propose an iterative procedure where the sequence alignment is re-iterated after training the classifier in such a way that the steps of classification and alignment inform each other. Peptides of core length 9 do not need to be re-aligned. The overall workflow with peptides of length 8-11 is as follows:
1. Align all sequences to a PWM profile of length 9.
2. Use this first alignment to train RBM-MHC (i.e. RBM and HLA-I classifier).
3. Re-align peptides based on the putative class predicted by the HLA-I classifier. First, for each HLA-I class, we put together the 10% of labeled data and the peptides classified in that class (weighted by the probability of classification) and we build on this sample a Hidden Markov Model (HMM) with the routines hmmprofstruct and hmmprofestimate (with default parameters) of the Matlab Bioinformatics Toolbox. We use these HMM profiles (essentially capturing the pattern of single-site residue conservation) as seeds of each class's alignment.
4. Use the HMM alignment score to correct classification errors coming from the initial, suboptimal alignment. We take every unlabeled peptide and we compute the HMM alignment score to each of the classes' seeds (by the hmmprofalign function). If the alignment score of a peptide is higher for a class that is different from the original, classifier-predicted one, we change the peptide alignment to the class with the higher alignment score, in such a way as to re-classify the peptide more accurately in the subsequent iteration.
5. Repeat the RBM and HLA-I classifier training after the re-alignment.
In the 10 "synthetic-individual" datasets considered for testing motif reconstruction (Supplementary Table  1) , we have observed a systematic improvement of the classification performance (measured by the indicators provided below) after 1 re-alignment step. For 2 or more iterations, there are cases in which the classification performance is degraded, therefore in our routine we re-align and re-train once.
Dataset collection from IEDB and preparation. Mass spectrometry datasets. A set of 8-11-mer peptides documented with HLA-A*02:01 restriction in the Immune Epitope Database (IEDB, last release [10] ), for a total of 5607 monoallelic-source sequences filtered as outlined below, was used to train the HLA-A*02:01specific RBM presentation model. We randomly selected a subset with the 80% of these sequences as training dataset and kept the remaining 20% as test set to evaluate the model's predictions in terms of probabilistic scores of presentation (Fig. 1c) . We randomly selected 5000 peptides from the human proteome (as in UniProt database [27] ) with length distribution matching the one of presented peptides to serve as a set of peptides predominantly not presented on the cell surface ("generic"). For motif reconstruction, we considered 10 "synthetic-individual" datasets of antigens with known HLA-I specificity to assess the RBM-MHC classification performance by comparing RBM-MHC predictions against the known HLA-I specificity. These datasets are built by collecting all IEDB antigens associated to 2 haplotypes, i.e. combinations of an A, a B, a C allele observed to co-occur in the human population (see Supplementary Table 1 ) in such a way as to preserve linkage. Information on haplotypes co-occurrence was found at allelefrequencies.net [28] . We prioritized HLA-specific peptides from monoallelic sources [4] in such a way that the assignment of HLA-binding specificity is unambiguous and does not rely on additional in silico predictions. The search was performed as follows. The full set of curated HLA-I ligands was downloaded from IEDB (file mhc_ligand_full.csv from http://www.iedb.org/database_export_v3.php, accessed in September 2019). We searched in IEDB for linear, human peptides of length 8-11 residues (as most of the HLA-I presented peptides have a length in this range). We looked for peptides eluted from cells (field in IEDB file Assay Group = "ligand presentation") and detected by mass spectrometry techniques (field Method/Technique = "cellular MHC/mass spectrometry", "mass spectrometry", "secreted MHC/mass spectrometry"). Among these data, we set the field Allele Evidence Code (describing the basis upon which the HLA-I assignment is made) to the most stringent option, "Single allele present". This definition indicates that antigen presenting cells are known to only express a single HLA-I molecule, as is the case for monoallelic cell lines [4, 5] . If for a given allele we found less than a minimal number (set to 300) of sequences among monoallelic-source data, we extended the search to peptides with evidence code "Allele Specific Purification", since this procedure attaches greater confidence to the HLA assignment than its inference by in silico methods. Only for one allele (HLA-B*39:01) we did extend the search to all other MS data, thus including the evidence code "Inferred by motif or alleles present". To apply the RBM-MHC method to patient-derived immunopeptidomic datasets, when we cannot retrieve a HLA annotation in IEDB for the 10% of their peptides, we add to each of them labeled peptides for the 6 HLA-I (given by the patient's HLA typing) collected from IEDB as above. The RBM-MHC training set is this minimally extended dataset, to guide the learning of the HLA-I classifier by the labeled peptides, whose predictions are used to reconstruct HLA-I motifs in the original dataset. We have not attempted to identify motifs on dataset Mel5 from [19] as the patient's HLA typing was uncomplete (lacking the HLA-C alleles).
Binding assay datasets. Records of binding assays from IEDB (as of March 2020) were filtered following [5] , i.e. selecting peptides annotated with a quantitative measure of binding dissociation constants < 500 nM and excluding assay types that led to documented discrepancies between predicted and effective affinity ("purified MHC/direct/radioactivity/dissociation constant KD", "purified MHC/direct/fluorescence/half maximal effective concentration (EC50)", "cellular MHC/direct/fluorescence/ half maximal effective concentration (EC50)").
MS-based validation of model predictions for neoantigen discovery. To evaluate the predictive power of the HLA-A*02:01-specific RBM model for neoantigen discovery, we acquired the list of missense mutations for ovarian and melanoma cancer cell lines SKOV3, A2780, OV90, HeLa and A375 from the Cosmic Cell Lines Project [29] as described in Ref. [14] , i.e. filtered by expression levels and by their affinity, estimated with NetMHCpan3.0 [30] , to the allele HLA*A:02:01 (known to expressed in these cell lines). The number of missense mutations is as follows: 93 (A375), 60 (SKOV3), 39 (A2780), 22 (OV90), 28 (HeLa), resulting in 38 possible peptides of length 8-11 containing each of them. We used the RBM log-likelihood L RBM as probabilistic score of presentation by HLA*A:02:01 to assign to all 8-11 residue long mutated peptides. Since the MS database for the 5 cancer lines produced by Ref. [14] was annotated by comparison to the consensus human proteome, it contained only wildtype peptides whose mutated version was in the list of putative neoantigens. As long as a mutation does not affect an anchor site, the mutated version should preserve a high probability of presentation, thus we have excluded from the analysis of Fig. 1d neoantigens arising from anchor-site mutations of MS-derived peptides.
The studies from which single-patient samples Mel15, Mel8, Mel5, 12T were retrieved [17] [18] [19] , listed a total of 11 tumour-presented neoantigens, using techniques that allow for the detection of variants of native proteins by MS. For some of these neoantigens, immunogenicity was also validated in vitro by identification of neoantigen-specific T-cell responses. Their HLA association was predicted by NetMHC. The prediction was to a large extent confirmed by in vitro validation of immunogenicity, which relied on antigen presenting cells that were positive to the predicted HLA-I. As the 11 neoantigens are 9-10 residues long, following Ref. [8] we considered 9-mer and 10-mer peptides overlapping the missense mutations observed in the patient's WES, which resulted in a list of thousands of putative neoantigens (see Supplementary Table  2 ). The RBM-MHC training set per se can thus take into account only 9-10 mers from patients' samples; patients' neoantigens to validate were not included in the training dataset. In this multi-allelic case where antigens may be presented by several HLA types (as opposed to the allele-specific case of the previous paragraph) we used the global score accounting for the probability of presentation (by RBM) as well as the confidence of the HLA assignment (either by the HLA-I classifier in RBM-MHC or by K-means clustering in RBM-km) of equation 9.
RBM-MHC performance at motif reconstruction in multi-allelic samples. We chose the Area Under the Curve (AUC) of the Receiving Operating Characteristic (ROC) as a metric for classification (i.e., HLA assignment) performance, estimated for each approach as follows. RBM-MHC, through the HLA-I classifier, outputs for each peptide a probability to belong to each HLA-I class. MixMHCp [8, 9] models probabilistically data by a mixture of independent models, thus it describes each sequence by a vector of "responsibilities", describing the probabilities to belong to each cluster (i.e. HLA-I class). NetMHC-pan4.0 [7] predicts peptide binding values from either the training on eluted data (EL option) or the binding affinity data (BA option) and it estimates from these values presentation scores and percentile rank scores. Low values of percentile rank define binders (following the authors' recommendations, peptides with percentile rank <2% and <0.5% are considered HLA-I weak and strong binders respectively). Having built the samples from MS data, we considered NetMHC predictions from the EL option (NetMHCpan4.0-EL). We compare the N c = 6 HLA-I classes by pairs. We consider the probability of belonging to a certain class that each method would assign to "positives" of that class (peptides binding to the respective HLA-I allele) and "negatives" (peptides binding to other alleles): when the classification performance is good, the former has values close to one, the latter has values close to zero. Varying the threshold between false positive and false negative distributions, we build the ROC curve. We take the area under the ROC curve (AUC) as measure of the ability to discriminate the two classes (AUC = 1 means perfect discrimination, AUC = 0.5 means chance). To obtain one cumulative indicator, we average over the AUCs of these pairwise comparisons. For the approaches partially or fully supervised (RBM-MHC and classifier-only), the AUC is measured only from data not in the 10% used for the supervised learning step.
To further assess classification performance, we looked also at the HLA type of each peptide predicted based on: the highest responsibility value among the N c values for MixMHCp; the lowest percentile rank for NetMHCpan-4.0 (EL option); the highest class probability estimated by the HLA-I classifier for RBM-MHC. In this way, when comparing peptides of different classes two by two, we can count true positives of classification (t p), false positives ( f p), true negatives (tn) and false negatives ( f n). Once these quantities are defined, additional classification performance indicators are: Accuracy=(t p + tn)/(t p + f p + tn + f n), Precision=t p/(t p + f p), Specificity=tn/(tn + f p), Sensitivity=t p/(t p + f n). MixMHCp [8, 9] was run using default options, which include an additional "trash cluster". When measuring these classification performance indicators, the assignment to the trash cluster is considered among the false negatives. The comparison of classification performance as measured by these indicators is shown in Supplementary Fig. 7 .
Evaluation of model predictions for SARS-CoV-2 epitopes. We downloaded the protein-coding regions of SARS-CoV-2 genome from GenBank, NCBI Reference Sequence: NC_045512.2. We extracted all the 9-mers contained in the SARS-CoV-2 proteome, giving a list of 9656 candidate cytotoxic T-cell epitopes (HLA-I antigens). Ref. [20] (available at https://www.immunitrack.com/free-coronavirus-report-fordownload/) tested in vitro the 94 top-scoring epitopes according to NetMHC4.0 [6] for each of the HLA-I alleles A*01:01, A*02:01, A*03:01, A*11:01, A*24:02, B*40:01, C*04:01, C*07:01, C*07:02 and according to NetMHCpan4.0 [7] for C*01:02. 159 peptides were identified as high-stability binders (i.e. with stability above the threshold of 60% of the reference peptide value for the corresponding HLA-I allele). To probe our method as a predictor for such binders, we learned a series of allele-specific RBM presentation models (5 hidden units, λ 2 1 = 0.001) for the 10 HLA-I alleles considered in this study. We decided to train (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. . https://doi.org/10.1101/2020.04.25.061069 doi: bioRxiv preprint a series of allele-specific models to minimize the uneven accuracy across alleles, emerging especially in multi-allelic models, due to the different abundances of peptides with different HLA-I preferences available as training data. (The use of multi-allelic models is intended as tool to characterize unannotated samples through motif reconstruction). Here, we prioritized training datasets from binding assays in IEDB (see above) to customize the presentation score toward the identification of high-affinity ligands. These types of data are almost absent for the 4 HLA-C mentioned, and in these cases models were learned from MS data only but a re-weighting of frequencies, aimed at correcting for MS biases in detection, was applied (see Supplementary Note). All peptides tested for binding to a given HLA-I in Ref. [20] were scored by the corresponding RBM model (see Fig. 2d , where, as a comparison, results achieved by training all RBMs on MS data with re-weigthing are also reported). We next estimated their score percentile relative to scores assigned to all candidate epitopes and we assessed that tested binders were predominantly assigned high scores, able to a good extent to discriminate them from the tested non-binders (see Supplementary Fig. 11) .
As an additional test, we considered the SARS-CoV-2 cytotoxic T-cell epitopes identified as potentially associated to high immune responses by Ref. [21] , who mapped the dominant, experimentally validated SARS-CoV-derived epitopes to the corresponding regions in the SARS-CoV-2 proteome. We scored the 22 epitopes in this list with complete (100%) or moderate-high (> 70%) sequence similarity to the homologous SARS-CoV epitope, which should preserve high likelihood of presentation. Since homologous SARS-CoV epitopes were experimentally tested in binding assays, scores were assigned by the same (affinity-trained) models as above covering the HLA restrictions reported in Ref. 2 (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. Table 1 ). As found in [1] , at low regularization (λ 2 1 = 0.001) the data representation is more intricate while a higher regularization (λ 2 1 = 0.1), by enforcing sparsity, gives non-zero weights to the amino acids and positions learnt as the most discriminative among "features" (here, HLA-I classes). The first λ 2 1 choice however retains more information about data, hence reaches a slightly higher classification performance when combined to the HLA-I classifier (see Supplementary Fig. 2c) ; the second λ 2 1 choice results in a more interpretable representation.
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. Figure 4 : HLA-I sequence motifs are captured through RBM local potentials and weights. The probability of presentation P(v) inferred by the RBM consists of potentials g i (v i ), describing single-site amino acid conservation, and terms describing the co-occurrence of motifs through the weights w iµ (v). In the first row, we consider a IEDB, MS-based dataset of antigens specific to one allele, HLA-B*51:01 (monoallelic dataset). A RBM model is trained on these data (chosen with 5 hidden units and λ 2 1 = 0.1, to enhance weights' interpretability, see Supplementary Fig. 3 ). With data of the same HLA-I type, the binding motif is essentially captured by conservation alone (logo representation of e ∑i(gi(vi)) , central column). Weights and hidden units embed additional data features (right column). Hidden unit 1 h 1 encodes the multiple binding modes of HLA-B*51:01 antigens discussed in [2] (see also Fig. 2c) : the sequence property "hydrophilic/charge" has a positive projection on W 1 (and hence positive inputs to h 1 ) while the pattern "hydrophobic" has a negative one. In the second row, we train a RBM (10 hidden units and λ 2 1 = 0.1) on a bi-allelic dataset, i.e. where we added to the monoallelic dataset IEDB, MS-derived antigen sequences specific to the allele HLA-A*03:01 (a minimal example of a sample mixing antigens of different specificities). In this setting: (i) overall conservation does not allow discrimination of HLA-I motifs (central column), for which the RBM data representation through weights and inputs to hidden units is necessary; (ii) this representation is robust with respect to the monoallelic case. To illustrate this, we have selected sets W 5 and W 6 (right column). They distinguish HLA-B*51:01 antigens with sequence patterns underlying alternative binding modes (pattern "hydrophilic/charge" has significant, negative projection on W 5 and is kept separate from pattern "hydrophobic", with a significant, positive projection on the other set of weights, W 6 ). HLA-B*51:01 sequence patterns are represented as mutually exclusive with respect to the sequence motif characteristic of HLA-A*03:01 (weights of opposite sign). As a result, antigens of different HLA-I types are clearly partitioned into 2 clusters in the space of inputs to h 5 and h 6 . The 2 subclusters in the HLA-B*51:01 one (blue points) correspond to its alternative binding modes. As conserved sites at position 9 play a role in representing HLA-B*51:01 vs HLA-A*03:01 specificity but not in discriminating HLA-B*51:01 binding modes, only in this bi-allelic case weights at that position have a non-negligible value.
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. Table 1 ) and represent the average over 10 realizations of the training.
Supplementary Figure 6 : AUC of performance of predicting antigen HLA association in "synthetic-individual" samples: same plot as in Fig. 1e for datasets containing peptides of length 8-11 residues, where a sequence alignment step is required (see Methods). "NetMHC" refers to the prediction by the version NetMHCpan4.0-EL (see Methods).
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. [3, 4] , sample size = 9262 sequences (restricted to length [8] [9] [10] [11] . Sequence motifs recovered by RBM-MHC are compared to the ones from monoallelic data in IEDB and the ones recovered by MixMHCp and NetMHCpan4.0-EL in the same sample. The quality of motif reconstruction is measured by ρ, the coefficient of correlation between single-site amino acid frequencies of the monoallelic IEDB motif and of the predicted one. The average ρ over the 6 HLA-I motifs recovered by the 3 methods is compared in Fig. 2b . Motifs of length 9 in monoallelic data are obtained by aligning sequences in each HLA-I class separately, using standard alignment routines as described in Methods. When comparing these motifs to the ones predicted by the different methods, we keep the alignment step consistent with the choice made for monoallelic data, i.e. for each method we align separately, applying the same standard alignment routines, the sequences assigned to the same HLA-I class. (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. Supplementary Figure 11 : RBM prediction of SARS-CoV-2 epitopes tested in vitro. RBM score percentile of putative SARS-CoV-2 ligands predicted by NetMHC (NetMHC4.0 and NetMHCpan4.0) for 10 HLA-I alleles, divided into "binders" and "non-binders" on the basis of the binding stability assigned in vitro by [10] (respectively > 60% and < 60% of reference values). RBM models were trained on binding assay (BA) data for HLA-A and HLA-B, on mass spectrometry (MS) data for HLA-C (see Methods). The average RBM score percentile for binders is consistently above 98 for each HLA-I allele. We quantify the RBM ability to distinguish binders from non-binders by the Receiver Operating Characteristic curve (inset), which describes, as a function of a score percentile threshold, how the fraction of binders and non-binders is recovered by the RBM score. Its Area Under the Curve (AUC) is significantly above the random expectation of 0.5. As a comparison, we added the AUC obtained using RBM models all trained on MS data, giving a comparable performance. An unbiased comparison to NetMHC is here difficult because peptides were pre-selected ahead of the experiments from the top 1% ranks estimated by NetMHC.
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The copyright holder for this preprint this version posted April 25, 2020. Test of the re-weighting procedure in a RBM model for presentation by HLA-A*01:01 trained on a set of MS data in IEDB (with ligand length in the range of 8-11 residues). b: Distribution of weights αs for ligands in the training dataset with and without C/W. c: Distribution of differences of presentation log-likelihoods defined as ∆L RBM (v) = L RBM (v) − L RBM val , where L RBM (v) = logP(v), · val stands for the average over the full validation dataset and v are sequences in the latter containing C/W. The 3 curves correspond to choosing either P = P MS (from the RBM model trained on MS data without re-weighting), or P = P MS reweight (from the RBM model trained on MS data with re-weighting) or P = P non−MS (from the RBM model trained on data obtained by techniques different from MS). d: Effect of re-weighting applied to the RBM model for presentation by HLA-A*02:01 of Figs. 1c-d (trained on IEDB, monoallelic MS-detected ligands of length 8-11 residues). Experimentally validated immunogenic peptides from T-cell recognition assays containing C/W (FIDSYICQV from [8] , SLWGGDVVL and FTWEGLYNV from [9] ) are assigned an equal or higher RBM presentation score by the re-weighted model (P MS reweight ) compared to the one without re-weighting (P MS ). Thanks to re-weighting, score percentiles, relative to the score distribution of the 5000 generic peptides considered in Figs. 1c-d, are all brought above 95%. RBM generative performance guided by the HLA-I classifier. The RBM model is generative. As it is based on fitting an entire probability distribution to a given dataset, sampling from this distribution allows one to generate new candidate antigens. The binding specificity to HLA-I of such generated sequences can be controlled by conditioning (fixing) the RBM probability on the values of inputs to hidden units coding for the desired specificity. The search for these values is guided by the HLA-I classifier. This procedure directly builds on the idea of sampling while conditioning on structural, functional, phylogenetic features emerging in RBM representations of protein families [1] . Schematically, the steps of this HLA-specific sampling are as follows: 1. we select a HLA-I class c (e.g. c = HLA-A*01:01) and we find I o such thatŷ c (I o ) is close to 1 (that is, there is high confidence of class c prediction); 2. we estimate h o = h from the conditional probability P(h|I o ) and we sample new sequences v from the probability of observed sequences conditioned on h o , P(v|h = h o ); 3. to further explore the region encoding for the specificity to HLA-I class c, we randomly move from I o to I n = I o + δ I (δ Is drawn from a Gaussian N (0, σ )); 4. we accept the move with a probability π ∼ exp (−β (e n − e o )), where e n = −log(ŷ c (I n )) and e o = −log(ŷ c (I o )). The parameter β (akin to an inverse temperature in physics) basically controls how stringent the selection for sequences predicted in class c with a high probability is. 5. We set I o = I n to proceed with new moves as in step 3. Every arbitrary amount of moves, we generate new configurations by conditional sampling as in step 2. Supplementary Fig. 14a shows examples of I n values (2 of its components for simplicity) covered in this search with β = 50 and σ = 0.01 (dark red points). By this sampling procedure, we generate samples of artificial peptides that would be predicted to be presentable and specifically presented by the selected HLA-I protein complex. Such samples explore a broad diversity of peptides, that are typically 3-4 mutations away from the closest natural ones (Supplementary Fig. 14b ) while preserving the profile of amino acid abundances constrained by a given binding specificity ( Supplementary Fig. 14c ). Fig. 1b . Inputs for antigen sequences with HLA-A*01:01 restriction (points in light red color) mark a region encoding for this binding specificity. We explore more systematically this region by making small, stochastic moves in this space (points in dark, red color) and by accepting them only if the classifier would predict with high confidence that the sampled antigens specifically bind to HLA-A*01:01 (see text). In this example, every 100 steps we condition on the current values of inputs to hidden units and sample 1000 configurations for a total of M g = 11000 generated sequences. b: Distribution of the diversity of the generated sequences with the selected HLA-A*01:01 specificity, quantified as number of mutations away from the closest natural sequence of the same specificity. c: Scatter plot comparing positional amino acid frequency of natural and generated antigen sequences with same binding specificity, HLA-A*01:01.
