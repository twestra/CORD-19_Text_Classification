The COVID-19 pandemic, caused by the SARS-CoV-2 virus, is leading to a burst of swiftly released scientific publications on the matter (1) . In response to the pandemic, many research groups have started projects to understand the SARS-CoV-2 virus life cycle and to find solutions. Examples of the numerous projects include outbreak.info (2) , VODAN around FAIR data (3) , CORD-19-on-FHIR (4) and #covidpathways (5) . Many research papers and preprints get published every week and many call for more Open Science (6) . The Dutch universities went a step further and want to make any previously published research openly available, in whatever way related to COVID-19 research (7) .
However, this swift release of research findings comes with an increased number of incorrect interpretations (8) which can be problematic when new research articles are picked up by main-stream media (9) . Rapid evaluation of these new research findings and integration with existing resources requires a frictionless access to the underlying research data upon which the findings are based. This requires interoperable data formats and sophisticated integration for these resources. A first step in this integration is knowing if you talk about the same gene or the same proteins. Using an interoperability layer in common, we can link different resources more easily.
The Gene Wiki project has been tearing down the different research silos on genetics, biological processes, related diseases and associated drugs (10) . In contrast to legacy databases, where data models follow a relational data schema of connected tables, Wikidata ( https://wikidata.org/ ) uses statements to store facts (see Figure 1 ) (10) (11) (12) (13) . This model of statements aligns well with the RDF triple model of the semantic web and the content of Wikidata is also serialized as Resource Description Framework (RDF) triples (14, 15) , acting as stepping stone for data resources to the semantic web. Through its SPARQL endpoint other nodes in the semantic web, using either mappings between these resources or through federated SPARQL queries (16) . Automated editing of Wikidata simplifies a lot of things, however, the quality control of that process must be monitored carefully. This requires a clear data schema that allows the various resources to be linked with additional provenance. This schema describes the key concepts required for the integrations of the resources we are interested in: NCBI Taxonomy (17) , NCBI Gene (18) , UniProt (19) , the Protein Data Bank (PDB) (20) , WikiPathways (21) , and PubMed. Therefore , the key elements for   which we need a model include viruses, virus strains, virus genes, and virus proteins. The first two provide the link to taxonomies, the models for genes and proteins link to UniProt, PDB, and WikiPathways. These key concepts are also required to annotate research output such as journal articles and datasets related to these topics. Wikidata calls such keywords 'main subjects'. The introduction of this model and the actual SARS-CoV-2 genes and proteins in Wikidata enables the integration of these resources.
This paper is a case report of a workflow/protocol for data integration and publication. The first step in this approach is to develop the data schema. Within
Wikidata, shape expressions (ShEx) are used as the structural schema language to describe and capture schema's of concepts (22, 23) . It shapes the RDF structure by which Wikidata content is made available. These shapes have the advantage that they are easily exchanged and describe linked data models as a knowledge graph.
Since the shapes describe the model, they enable discussion, revealing inconsistencies between resources and allow for consistency checks of the content added by automated procedures.
After introducing the model, the process of adding knowledge to Wikidata is described. In this process, the seven human coronaviruses (HCoVs), MERS, SARS, Wikidata. This protocol is finalised by describing how the resulting data schema and data can be applied to support other projects, particularly #covidpathways .
The semantic web was proposed as a vision of the Web, in which information is given well-defined meaning and better enabling computers and people to work in cooperation (25) . In order to achieve that goal, several technologies have appeared, like RDF for describing resources (15) , SPARQL to query RDF data (26) and the Web Ontology Language (OWL) to represent ontologies (27) .
Linked data was later proposed as a set of best practices to share and reuse data on the web (28) . The linked data principles can be summarized in four rules that promote the use of uniform resource identifiers (URIs) to name things, which can be looked up to retrieve useful information for humans and for machines using RDF, as well as having links to related resources. These principles have been adopted by several projects, enabling a web of reusable data, known as the linked data cloud ( https://lod-cloud.net/ ), which has also been applied to life science (29) .
One prominent project is Wikidata, which has become one of the largest collections of open data on the web (16) . Wikidata follows the linked data principles offering both HTML and RDF views of every item with their corresponding links to related items, and a SPARQL endpoint called the Wikidata Query Service. Wikidata's RDF model offers a reification mechanism which enables to represent information about statements like qualifiers and references (see also That statement can be reified to add qualifiers and references. For example, a qualifier can state that the genomic assembly ( P659 ) is GRCh38 ( Q20966585 ) with a reference declaring that it was stated ( P248 ) in ensembl Release 99 ( Q83867711 ).
In Turtle, those declarations are represented as (see also wd: Q29867336 rdfs: label "human X chromosome" . wd: Q20966585 rdfs: label "Genome assembly GRCh38" . wd: Q83867711 rdfs: label "ensembl Release 99" .
Although the RDF data model is very flexible, specifying an agreed structure for the data allows domain experts to identify the properties and structure of their data facilitating the integration between heterogeneous data sources. Shape expressions were used to provide the right level of abstraction. YaShE, a ShEx editor implemented in JavaScript, was applied to author these shapes (32) . This application provides the means to associate labels in the natural language of Wikidata to the corresponding identifiers. The initial entity schema was defined with YaShE as a proof of concept for virus genes and proteins. In parallel, statements already available in Wikidata were used to automatically generate an initial shape for virus strains with sheXer. The statements for virus strains were retrieved with SPARQL 6 . CC-BY 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint from the Wikidata Query Service (WDQS). The generated shape was then further improved through manual curation. The syntax of the shape expressions were continuously validated through YaShE and the Wikidata Entity Schema namespace was used to share and collaboratively update the schema with new properties. Figure 3 gives a visual outline of these steps. 
The second step in our workflow is to add entries for all virus strains, genes and their gene products to Wikidata. This information is spread over different resources. Here, annotations were obtained from NCBI EUtils (33) , Mygene.info (34) , and UniProt, as 7 . CC-BY 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint outlined below. Input to the workflow is the NCBI Taxonomy identifier of a virus under scrutiny. ( e.g. 2697049 for SARS-CoV-2). The taxon annotations are extracted from NCBI EUtils. The gene and gene product annotations are extracted from mygene.info and the protein annotations are extracted from UniProt using the SPARQL endpoint at ( https://sparql.uniprot.org/ ). Genomic information from seven human coronaviruses (HCoVs) was obtained from literature including the NCBI Taxonomy identifiers. For six virus strains a reference genome was available and was used to populate Wikidata. For SARS-CoV-1, the NCBI Taxonomy identifier referred to various strains, though no reference strain was available. To overcome this issue, the species taxon for SARS-related coronaviruses (SARSr-CoV) was used instead, following the practices of NCBI Genes and UniProt.
The Entrez Programming Utilities (EUtils) is the application programming interface (API) to the Entrez query and database system at the National Center for Biotechnology Information (NCBI). From this set of services the scientific name of the virus under scrutiny was extracted (e.g. "Severe acute respiratory syndrome coronavirus 2").
Mygene.info is a web service which provides a REST API that can be used to obtain up-to-data gene annotations. The first step in the process is to get a list of applicable genes for a given virus by providing the NCBI taxon id. The following step is to obtain The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint EUtils. UniProt Identifiers are acquired using the SPARQL endpoint of UniProt, which is a rich resource for protein annotations provided by the Swiss Bioinformatics Institute. Figure 5 shows the SPARQL query that was applied to acquire the protein annotations.
{ "_id": "43740571" , "_score": 15.594226 , "accession": { "genomic": [ "MN908947.3" , "NC_045512.2" ], "protein": [ "QHD43419.1" , "YP_009724393.1" ] }, "entrezgene": "43740571" , "locus_tag": "GU280_gp05" , "name": "membrane glycoprotein" , "other_names": "membrane glycoprotein" , "refseq": { "genomic": "NC_045512.2" , "protein": "YP_009724393.1" }, "retired": 43560233 , "symbol": "M" , "taxid": 2697049 , "type_of_gene": "protein-coding" } The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint updated regularly and will allow pathway curators to annotate virus genes and proteins in their pathways and provide linkouts on the WikiPathways website.
The COVID-19 related pathways from WikiPathways COVID-19 Portal are added to Wikidata using the approach previously described (10) . For this, a dedicated repository has been set up to hold the GPML files, the internal WikiPathways file format, The GPML is converted into RDF files with the WikiPathways RDF generator (36) , while the files with author information are manually edited. For getting the most recent GPML files, a custom Bash script was developed (getPathways.sh). The conversion of the GPML to RDF uses the previously published tools for WikiPathways RDF (36) . Here, we adapted the code with unit tests to have one for each GPML file, using some simple template test. These are available in the SARS-CoV-2-WikiPathways branch of Wikidata2Bridgedb . Based on this earlier generated pathway RDF and using the Wikidataintegrator library, the WikiPathways bot was used to populate Wikidata with additional statements and items. The pathway bot was extended with the capability to link virus proteins to the corresponding pathways which was essential to support the Wikidata resource.
These changes can be found in the sars-cov-2-wikipathways-2 branch.
The second use case is to demonstrate how we can link virus gene and protein information to literature. Here, we used Scholia ( https://tools.wmflabs.org/scholia/ ) as a central tool (13) . It provides a graphical interface around data in Wikidata, for example literature about a specific coronavirus protein (e.g. Q87917585 for the SARS-CoV-2 spike protein). Scholia uses SPARQL queries to provide information about topics. We annotated literature around the HCoVs with the specific virus strains, the virus genes, and the virus proteins as 'main topic'.
To align the different sources in Wikidata, a common data schema is needed. We have created a collection of schema's that represent the structure of the items added 11 . CC-BY 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint to wikidata. Input to the workflow is the NCBI taxon identifier, which is input to mygene.info (see Figure 3 ). Taxon information is obtained and added to Wikidata according to a set of linked Entity Schemas ( E170 : virus, E174 : strain, E69 : disease). Gene annotations are obtained and added to WIkidata following the Schemas ( E165 : virus gene, E169 : virus protein) and protein annotations are obtained and added to Wikidata following the two schemas. The last two schemas are an extension from more generic schemas for proteins ( E167 ) and genes ( E75 ). Table 1 . The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint Wikidata identifiers, 69 NCBI Gene identifiers, 42 UniProt identifiers, and 55 RefSeq identifiers. The mapping file has been released on the BridgeDb website ( https://bridgedb.github.io/data/gene_database/ ). The mapping database has also been loaded on the BridgeDb webservice at http://webservice.bridgedb.org/ which means it can be used in the next use case: providing links out for WikiPathways. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint gene and protein, two Wikidata identifiers with links may be given. In that case, one is for the gene and one for the protein.
The WikiPathways use case shows us that literature describes our knowledge about how coronaviruses work at a rather detailed level. Indeed, many articles discuss the genetics, homology of genes and proteins across viruses, or the molecular aspects of how these proteins are created and how they interfere with the biology of the human cell. The biological pathways show these processes, but ultimately the knowledge comes from literature. Wikidata allows us to link literature to specific virus proteins and genes, depending on what the article describes. For this it uses the 'main subject' property ( P921 ). We manually annotated literature with the Wikidata items for specific proteins and genes. We developed two SPARQL queries to count the number of links between genes ( https://w.wiki/Lsp ) and proteins ( https://w.wiki/Lsq ) and the articles that discuss them. Scholia takes advantage of the 'main subject' annotation, allowing the creation of "topic" pages for each protein. For example, Figure 8 shows the topic page of the SARS-CoV-2 spike protein. Wikidata provides a solution. It is part of the semantic web, taking advantage of its reification of the Wikidata items as RDF. Data in Wikidata itself is frequently, often almost instantaneously, synchronised with the RDF resource and available through its SPARQL endpoint ( http://query.wikidata.org ). The modelling process turns out to be an important aspect of this protocol. Wikidata contains numerous entity classes as entities and more than 7000 properties which are ready for (re-)use. However, that also means that one is easily lost. The ShEx schema have helped us develop a clear model, as a social contract between the authors of this paper, as well as documentation for future users.
Using these schemas, it was simpler to validate the correctness of the updated bots to enter data in Wikidata. The bots have been transferred to the Gene Wiki Jenkins platform. This allows the bots to be kept running regularly, pending the ongoing efforts of the coronavirus and COVID-19 research communities. While the work of the bots will continue to need human oversight, potentially to correct errors, it provides a level of scalability and generally reduces the authors from a lot of repetitive work.
One of the risks of using bots, is the possible generation of duplicate items. This can happen to humans as well, but they can apply a wider range of academic skills to resolve these issues. Indeed, in running the bots, duplicate Wikidata items were created, for which an example is shown in Figure 9 . The Wikidataintegrator library does have functionality to prevent the creation of duplicates by comparing properties, based on used database identifiers. However, if two items have been created using different identifiers, these cannot be easily identified. Close inspection of examples, such as the one in Figure 9 , showed that the duplicates were created because there was a lack of overlap between the data to be added and the existing item. The UniProt identifier did not yet resolve, because it was manually extracted from information in a March 27 pre-release (see ftp.uniprot.org/pub/databases/uniprot/pre_release/ ). The Pfam identifier pointed to a page that did not contain mappings to other identifiers. In addition, the lack of references to the primary source hampers curators ability to merge duplicate items and expert knowledge was essential to identify the duplication. Fortunately, names used for these RNA viruses only refer to one protein as the membrane protein.
Generally, the curator would have to revert to the primary literature to identify the 17 . CC-BY 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.04.05.026336 doi: bioRxiv preprint overlap. Statements about 'encoded by' to the protein coding genes were found to be helpful as well. Reconciliation might be possible through sequence alignment, which means substantial expert knowledge and skills are required.
This makes reconciliation in Wikidata based on matching labels, descriptions and synonyms, matching statements and captured provenance (qualifiers and references) hazardous, due to different meanings to the same label. A targeted curation query ( geneAndProteinCurationQuery.rq , see Supporting Information) was developed to highlight such duplications and manually curated seven duplicate protein entries for SARS-CoV-2 alone. This duplication is common and to be expected, particularly in situations like a pandemic, when many groups contribute to the same effort. In this case, this paper only represents one group contributing to the Wikidata:WikiProject COVID-19 .
We also discovered that the virus taxonomy is different from those of mammalian species. For example, there is no clear NCBI taxon identifier for SARS-CoV-1 and after consultation with other projects, we defaulted to using the taxon identifier for the SARS-related CoVs, something that NCBI and UniProt seem to have done as well.
Finally, we note that during the two weeks this effort took place, several other resources introduced overviews, including dedicated COVID-19 portals from UniProt 
