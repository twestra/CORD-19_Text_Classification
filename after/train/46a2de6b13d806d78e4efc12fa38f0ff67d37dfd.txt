To mine Twitter to quantitatively analyze COVID-19 symptoms self-reported by users, compare symptom distributions against clinical studies, and create a symptom lexicon for the research community.
We retrieved tweets using COVID-19-related keywords, and performed several layers of semi-automatic filtering to curate self-reports of positive-tested users. We extracted COVID-19-related symptoms mentioned by the users, mapped them to standard IDs, and compared the distributions with multiple studies conducted in clinical settings.
We identified 203 positive-tested users who reported 932 symptoms using 598 unique expressions. The most frequently-reported symptoms were fever/pyrexia (65%), cough (56%), body aches/pain (40%), headache (35%), fatigue (35%), and dyspnea (34%) amongst users who reported at least 1 symptom. Mild symptoms, such as anosmia (26%) and ageusia (24%) were frequently reported on Twitter, but not in clinical studies.
The spectrum of COVID-19 symptoms identified from Twitter may complement those identified in clinical settings.
The outbreak of the coronavirus disease 2019 (COVID-19) is now considered to be a global pandemic 1 and is estimated to be one of the worst pandemics in the known World history. 2 At the time of the writing of this article, over 2 million confirmed positive cases have been reported globally, with over 125,000 deaths. 3 As the pandemic continues to ravage the world, there is a flurry of research activities are being conducted whose focuses range from trialing possible vaccines and predicting the trajectory of the outbreak to exploring the characteristics of the virus better by studying those infected.
Studies focused on identifying the symptoms experienced by those infected by the virus have typically been conducted through patients who were hospitalized or received clinical care. [4] [5] [6] Many infected people only experience mild symptoms or are asymptomatic and do not seek clinical care, although the specific portion of asymptomatic carriers is unknown. [7] [8] [9] To better understand the full spectrum of symptoms experienced by infected people, there is a need to look beyond hospital-or clinic-focused studies.
With this in mind, we explored the possibility of using social media, namely Twitter, to study symptoms self-reported by users who tested positive for COVID-19. Our primary goals were to (i) verify that users report their experiences with COVID-19-including their positive test results and symptoms experiencedon Twitter, and to (ii) compare the distribution self-reported of symptoms with those reported in studies conducted in clinical settings. Our secondary objectives were to (i) create a COVID-19 symptom corpus that captures the multitude of ways in which users express symptoms so that natural language processing (NLP) systems may be developed for automated symptom detection, and (ii) collect a cohort of COVID-19-positive Twitter users whose longitudinal self-reported information may be studied in the future. To the best of our knowledge, this is the first study that focuses on extracting COVID-19 symptoms from public social media. We will make the symptom corpus available with this paper to assist the research community, and it will be part of a larger, maintained data resource-a social media COVID-19 Data Bundle (https://sarkerlab.org/covid_sm_data_bundle/).
We collected data from Twitter via the twitter public streaming application programming interface (API).
First, we used a set of keywords/phrases related to the coronavirus to detect tweets through the API: covid, covid19, covid-19, coronavirus, and corona AND virus, including their hashtag equivalents (e.g., #covid19).
Due to the high global interest in this topic, these keywords retrieved very large numbers of tweets.
Therefore, we applied a first level of filtering to only keep tweets that also mentioned at least one of the following terms: positive, negative, test and tested, along with one of the personal pronouns I, my, us, we, . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)
The copyright holder for this preprint this version posted April 22, 2020 10 and applied the same layers of filers. This provided us with a more manageable set of manual review, although most were still false positives (e.g., ' … I dreamt that I tested positive for covid …'). We manually reviewed the tweets to identify true selfreports, while discarding the clear false positives. We found some users falsely reporting positive tests, copying posts by celebrities or initially claiming positive tests only to clarify in later tweets that the tests actually came back negative, and we removed these users from our COVID-19-positive set. These multiple layers of filtering gave us a manageable set of potential COVID-19-positive users whose tweets we could analyze semi-automatically. The filtering decisions were made on iteratively-by collecting sample data for hours and days, and then updating the collection strategy based on analyses of the collected data.
For all the COVID-19-positive users identified, we collected all their past posts dating back to February 1, 2020. We did not include tweets earlier than that because we assumed that symptoms posted prior to that would not be related to COVID-19, particularly because our data collection started in late February and most of the positive test announcements we detected were from late March to early April. Since we were interested in identifying patient-reported symptoms only in this study, we tried to shortlist tweets that were likely to mention symptoms. To perform this, we first created a meta-lexicon by combining MedDRA, 11 Consumer Health Vocabulary (CHV), 12 and SIDER. 13 Lexicon-based approaches are known to have low recall particularly for social media data since social media expressions are often non-standard and contain misspellings. 14,15 Therefore, instead of searching the tweets for exact expressions from the tweets, we performed inexact matching using a string similarity metric. Specifically, for every symptom in the lexicon, we searched windows of sequences of characters in each tweet that had similarity values above a specific threshold. We used the levenshtein ratio as the similarity metric, computed as 1 − . .
, where lev.
dist. represents the levenshtein distance between the two strings and max(length) represents the length of the longer string. Our intent was to attain high recall, so that we were unlikely to miss possible expressions of symptoms while also filtering out many tweets that were completely off topic. We set the threshold via trial and error over sample tweets, and because of the focus on high recall, this approach still retrieved many false positives (e.g., tweets mentioning body parts but not in the context of an illness or a symptom). After running this inexact matching approach on approximately 50 user profiles, we manually extracted the true positive expressions (i.e., those that expressed symptoms in the context of a COVID-19) and added them . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint this version posted April 22, 2020. . https://doi.org/10.1101/2020.04. 16.20067421 doi: medRxiv preprint to the existing lexicons. We also manually reviewed all the tweets from these profiles to ensure that our similarity-matching approach did not miss any possible symptoms.
Following these multiple filtering methods, we manually reviewed all the posts from all the users, identified each true symptom expressed, and removed the false positives. We semi-automatically mapped the expressions to standardized concept IDs in the Unified Medical Language System (UMLS) using the meta-lexicon we developed and the NCBO BioPortal. 16 In the absence of exact matches, we searched the BioPortal to find the most appropriate mappings. For user profiles that had too few or no tweets remaining after the filtering processes, we manually checked their Twitter profiles using the web interface. All annotations and mappings were reviewed, reviewers' questions were discussed at meetings. In general, we found that it was easy for annotators to detect expressions of symptoms, even when the expressions were non-standard ('pounding in my head' = Headache).
Following the mapping process, we computed the frequencies of the patient-reported symptoms on
Twitter and compared them with several other recent studies that used data from other sources. We also identified users who reported that they had tested positive and also specifically stated that they showed 'no symptoms'. When computing the frequencies and percentages of symptoms, we used two models: (i) computing raw frequencies over all the detected users, and (ii) computing frequencies for only those users who reported at least 1 symptom or explicitly stated that they had no symptoms. We believe the frequency distribution for (ii) was more robust as for users who reported no specific symptoms, we could not verify if they had actually experienced any symptoms and not reported them or just did not share symptoms over Twitter.
Our initial keyword-based data collection and filtering from the different sources retrieved millions of tweets, excluding retweets, and we found repeated tweets to be a major problem. This resulted from celebrities or verified users with many followers posting about their own experiences with COVID-19, and then many users re-posting (not retweeting) the exact texts. Removing such duplicate tweets left us with 305 users (472,018 tweets) to review. 102 of them were labeled as 'negatives'-users who had specifically stated that their tests had come back negative, who removed their original posts about testing positive, or whose positive-test-indicating posts appeared to be fake or inconclusive after a detailed review of the account (e.g., we found some users claiming they tested positive as an April fool joke). This left us with 203 true positive users, with 117,572 tweets, whose positive testing posts were deemed to be legitimate.
The symptom detection approach reduced the number of unique tweets to review to 6,438.
. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint this version posted April 22, 2020. . https://doi.org/10.1101/2020.04. 16.20067421 doi: medRxiv preprint The 203 users expressed 932 total symptoms (mean: 4.59 per user; median: 4) using 598 unique expressions, which we grouped into 48 categories (Table 1) . We computed two sets of percentages-for all users (n=203) and for users who expressed at least 1 symptom or stated that they were asymptomatic (n=169). 34 users did not did not express any specific symptoms, while 10 users explicitly mentioned that they experienced no symptoms. As can be seen from the table, fever/pyrexia was the most commonly reported symptom, followed by cough, body ache & pain, headache, fatigue, dyspnea, chills, anosmia, ageusia, throat pain and chest pain, with each mentioned by 20% of the users who reported at least one symptom. Figure 1 illustrates the distribution of the number of symptoms reported by the users in our cohort. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint this version posted April 22, 2020 . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint this version posted April 22, 2020. . https://doi.org/10.1101/2020.04. 16.20067421 doi: medRxiv preprint . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
The copyright holder for this preprint this version posted April 22, 2020.  Reported as nasal congestion. Table 2 compares the symptom percentages reported by our Twitter cohort with several studies conducted in clinical settings (i.e., patients who were either hospitalized or visited hospitals/clinics for treatment). The top symptoms remained fairly consistent across the studies-fever/pyrexia, cough, dyspnea, headache, body ache and fatigue. The percentage of fever (65%), though the highest in our dataset, is lower than all the studies conducted in clinical settings. In our study, we distinguished, where possible, between myalgia and arthralgia, and combined pain (general) and body ache. Combining all these into one category, as some studies had done, would result in a higher proportion. We found considerable numbers of reports of anosmia (26%) and ageusia (24%), with approximately one-fourth of our cohort reporting these symptoms.
Reports of these symptoms, however, were missing from the other studies.
Our study revealed that there were many self-reports of COVID-19 positive tests on Twitter, although such reports are buried in large amounts of noise. We observed a common trend among Twitter users of describing their day-to-day disease progression since the onset of symptoms. This trend perhaps became popular as celebrities started describing their symptoms on Twitter. We saw many reports from users who tested positive but initially showed no symptoms, and some who expressed anosmia and/or ageusia as the only symptoms, which were documented in the comparison studies. There are some studies that suggest that anosmia and ageusia may be the only symptom of COVID-19 among otherwise asymptomatic patients. [20] [21] [22] The most likely explanation behind the differences between symptoms reported on Twitter versus clinical studies is that the former were reported mostly by users who had milder infections, while people who visited hospitals often went there to receive treatment for more serious symptoms. Also, the median ages of the patients studied in clinical studies tended to be much higher than the median age of Twitter users (in the U.S., median Twitter user age is 40 23 ). In contrast to the clinical studies, in our cohort, some users expressed stress and anxiety suffered after testing positive. It was difficult in many cases to ascertain if the mental health issues were directly related to COVID-19 or whether the users had prior histories of such conditions.
To the best of our knowledge, this is the first study to have utilized Twitter to curate symptoms posted by COVID-19-positive users. In the interest of community-driven research, we will be making the symptom lexicon available with this publication. The cohort of users detected over social media will enable us to conduct further studies in the future, enable us to study relatively unexplored topics such as the mental health impacts of the pandemic, and the long-term health of those infected by the virus.
. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)
The copyright holder for this preprint this version posted April 22, 2020. . https://doi.org/10.1101/2020.04. 16.20067421 doi: medRxiv preprint . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)
The copyright holder for this preprint this version posted April 22, 2020. . https://doi.org/10.1101/2020.04. 16.20067421 doi: medRxiv preprint 
