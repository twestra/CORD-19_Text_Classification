Health planners use forecasts of key metrics associated with influenza-like-illness (ILI); near-term weekly incidence, week of season onset, week of peak, and intensity of peak. Here, we describe our participation in a weekly prospective ILI forecasting challenge for the United States for the 2016-17 season and subsequent evaluation of our performance. We implemented a metapopulation model framework with 32 model variants. Variants differed from each other in their assumptions about: the force-of-infection (FOI); use of uninformative priors; the use of discounted historical data for not-yet-observed time points; and the treatment of regions as either independent or coupled. Individual model variants were chosen subjectively as the basis for our weekly forecasts; however, a subset of coupled models were only available part way through the season. Most frequently, during the 2016-17 season, we chose; FOI variants with both school vacations and humidity terms; uninformative priors; the inclusion of discounted historical data for not-yet-observed time points; and coupled regions (when available). Our near-term weekly forecasts substantially over-estimated incidence early in the season when coupled models were not available. However, our forecast accuracy improved in absolute terms and relative to other teams once coupled solutions were available. In retrospective analysis, we found that the 2016-17 season was not typical: on average, coupled models performed better when fit without historically augmented data. Also, we tested a simple ensemble model for the 2016-17 season and found that it underperformed our subjective choice for all forecast targets. In this study, we were able to improve accuracy during a prospective forecasting exercise by coupling dynamics between regions. Although reduction of forecast subjectivity should be a long-term goal, some degree of human intervention is likely to improve forecast accuracy in the medium-term in parallel with the systematic consideration of more sophisticated ensemble approaches.
It is estimated that there are between 3 and 5 million worldwide annual seasonal cases 1 of severe influenza illness, and between 290 000 and 650 000 respiratory deaths [1].
2 Influenza-like-illness (ILI) describes a set of symptoms and is a practical way for 3 health-care workers to easily estimate likely influenza cases. The Centers for Disease 4 Control (CDC) collects and disseminates ILI information, and has, for the last several 5 years, run a forecasting challenge (the CDC Flu Challenge) for modelers to predict 6 near-term weekly incidence, week of season onset, week of peak, and intensity of peak. 7 We have developed a modeling framework that accounts for a range of mechanisms 8 thought to be important for influenza transmission, such as climatic conditions, school 9 vacations, and coupling between different regions. In this study we describe our forecast 10 procedure for the 2016-17 season and highlight which features of our models resulted in 11 better or worse forecasts. Most notably, we found that when the dynamics of different 12 regions are coupled together, the forecast accuracy improves. We also found that the 13 most accurate forecasts required some level of forecaster interaction, that is, the 14 procedure could not be completely automated without a reduction in accuracy.
PLOS 2/43 . CC-BY 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint Infectious pathogens with short generation times pose public health challenges because 17 they generate substantial near-term uncertainty in the risk of disease. This uncertainty 18 is most acute and shared globally during the initial stages of emergence of novel human 19 pathogens such as SARS [2], pandemic influenza [3], or the Zika virus [4] . However, at 20 national and sub-national levels, uncertainty arises frequently for epidemic pathogens 21 such as seasonal influenza, dengue, RSV and rotavirus; causing problems both for 22 health planners and at-risk individuals who may consider changing their behavior to 23 mitigate their risk during peak periods. 24 Seasonal influenza affects populations in all global regions and is forecast annually in 25 temperate populations, either implicitly or explicitly [5]. Peak demand for both 26 outpatient and inpatient care is driven by peak incidence of influenza in many years [6]. 27 Therefore, the efficient provision of elective procedures and other non-seasonal health 28 care can be improved by accurate forecasts of seasonal influenza. Implicitly, most 29 temperate health systems use knowledge of historical scenarios with which to plan for 30 their influenza season. The current situation is then assessed against the deviation from 31
the historical averages and worst-cases as observed in their own surveillance system.
The United States Centers for Disease Control (CDC) has sought to formalize regional 33 and national forecasts by introducing an annual competition [7] . Each week, 34 participating teams submit weekly estimates of incidence for the next four weeks, season 35 onset, and timing and intensity of the peak. Methods used by teams include purely 36 statistical models, [8-10] mechanistic models [11, 12] machine learning and hybrid 37 approaches [13-15]. Expert-opinion surveys have also been used and performed well.
38 Some teams augment their forecasts of the official ILI data with the use of potentially 39 faster or less-noisy datasets such as google flu trends [16].
40 Here we describe a our mechanistic-model-supported participation in the 2016-17 CDC 41 influenza forecasting challenge, as an example of a disease forecasting process. We 42 emphasize a subjective human component of this process and also describe a 43 retrospective evaluation of the models for the previous six seasons. All the models 44 PLOS 3/43 . CC-BY 4.0 International license is made available under a
The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint described are implemented in the R package Dynamics of Interacting Community 45 Epidemics (DICE, https://github.com/predsci/DICE). 46 Methods 47 Data 48 The CDC Influenza-like-illness Surveillance Network (ILINet) Human and Health 49 Services (HHS) region and national data were downloaded from the CDC-hosted web 50 application FluView [17] and used to create a historic database of ILI cases. Fig S1 51
shows which states are grouped into each HHS region, along with the population of each 52 region. Because we require an absolute number of cases per week, the CDC ILINet data 53 is converted from percent ILI cases per patient to ILI cases. We estimate the absolute 54 number of weekly ILI cases by dividing the weighted percent of ILI cases in the CDC 55 data by 100 and multiplying it by the total weekly number of patients. We assume two 56 outpatient visits per person per year so that the total weekly number of patients is 57 estimated as: (total regional population)x(2 outpatient visits per person per year)x(1 58 year/52 weeks). 59 The estimate of two outpatient visits per year is based on two studies. In 2006 60 Schappert and Burt [18] studied the National Ambulatory Medical Care Surveys 61 (NAMCS) and the National Hospital Ambulatory Medical Care Surveys (NHAMCS) 62 and calculated an ambulatory rate of 3.8 visits per capita-year. The 2011 NAMCS [19] 63 and NHAMCS surveys [20] estimated ambulatory visit rates of 3.32 visits per capita per 64
year for physician's offices, 0.43 for hospital emergency departments and 0.33 hospital 65 Outpatient Departments. We sum these rates to get an outpatient visit per capita-year 66 of 4.08. We further estimate from the surveys that only half of these outpatient clinics 67 are sites that report to ILINet, and hence we rounded to our two outpatient visits per 68 year estimate. 69 Specific humidity (SH) is measured in units of kg per kg and is defined as the ratio of 70 PLOS 4/43 . CC-BY 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint water vapor mass to total moist air mass. Two other measurements of humidity are 71 absolute humidity and relative humidity. SH is included in DICE as a potential modifier 72 of transmissibility for this time period and uses Phase-2 of the North American Land 73 Data Assimilation System (NLDAS-2) data base provided by NASA [21-23]. The 74 NLDAS-2 data base provides hourly specific humidity (measured 2-meters above the 75 ground) for the continental US at a spatial grid of 0.125 • which we average to daily and 76 weekly values. The weekly data is then spatially-averaged for the states and CDC 77 regions. 78 School vacation schedules were collected for the 2014-15 and 2015-16 academic years for 79 every state. For each state, a school district was identified to represent each of the three 80 largest cities. Vacation schedules were then collected directly from the district websites. 81 These three school vacation schedules were first processed to a weekly schedule with a 82 value of 0 indicating class was in session all five weekdays and a 1 indicating five 83 vacation days. Next, the representative state schedule was produced by averaging the 84 three weekly district schedules. Region schedules are obtained by a population-weighted 85 average of the state schedules. Similarly, the national schedule is generated by a 86 population-weighted average of the regions. 87 For the 2016-17 season we determine start and end times as well as spring and fall 88 breaks from the previous years schedules. Thanksgiving and winter vacation timing was 89 taken from the calendar where the winter break is assumed to be the last two calendar 90 weeks of the year. Based on the proportion of schools closed and number of days closed, 91 p(t) is assigned a value in the range [0, 1]. For example in week t i , if all schools are 92 closed for the entire week then we define the proportion of open schools p(t i ) = 1. 93 However, if all schools have Monday and Tuesday off (missing 2 of 5 days), then 94 p(t i ) = 0.4. Similarly, if 3 of 10 schools have spring break (entire week off), but the 95 other 7 schools have a full week of class then p(t i ) = 0.3. If all schools have a full week 96 of class then p(t i ) = 0.
Basic model 98 The DICE package has been designed to implement meta-population epidemic modeling 99 on an arbitrary spatial scale with or without coupling between the regions. Our model 100 for coupling between spatial regions follows ref [24] . We assume a system of coupled 101 S-I-R equations (susceptible-infectious-recovered) for each spatial region. In this 102 scenario, the rate at which a susceptible person in region j becomes infectious (that is 103 transitions to the I compartment in region j) depends on: (1) the risk of infection from 104 those in the same region j, (2) the risk of infection from infected people from region i 105 who traveled to region j, and (3) the risk of infection encountered when traveling from 106 region j to region i. To account for the three mechanisms of transmission, ref [24] 107 defined the force of infection, or the average rate that susceptible individuals in region i 108 become infected per time step as:
where D is the total number of regions. In our case, unlike reference [24] , the 110 transmissibility is not the same for all regions and it is allowed to depend on time: β j (t). 111
Given this force of infection we can write the coupled S-I-R equations for each region as:
in the mobility matrix is one and in the limit of no mobility between regions the mobility matrix m ij is the identity matrix so that λ i (t) = β i (t) Ii Ni and we recover the 118 familiar (uncoupled) S-I-R equations:
The level of interaction between spatial regions is determined by the mobility matrix 120 and its interaction kernel, κ(r ij ):
This kernel is expected to depend on the geographic distance between the regions (r ij ), 122
and following Mills and Riley [24] we use a variation of the off-set power function for it: 123 κ(r ij ) = 1 1 + (r ij /s d ) γ (9) where s d is a saturation distance in km and the power γ determines the amount of 124 mixing between the regions: as γ decreases there is more mixing while as γ increases, 125 mixing is reduced. In the limit that γ → ∞ there is no mixing between regions and we 126 recover the uncoupled SIR Eqs. (5) (6) (7) . The DICE package is designed to allow the λ j (t)S j (or β j (t)
Nj in the uncoupled case):
scaling by percent clinical p C j , and adding a baseline B j . The term p C j is the proportion 133 of infectious individuals that present themselves to a clinic with ILI symptoms and B j 134 is a constant that estimates non-S-I-R or false-ILI cases. The integral runs over one 135 week determining the number of model cases for week t i . ∆ t approximates the time 136 delay from when an individual becomes infectious to when they visit a sentinel provider 137
for ILI symptoms and is set to 0.5 weeks based on prior calibration [25, 26] . Eq. 10 138 describes how DICE relates its internal, continuous S-I-R model to the discrete ILI data. 139
In the next section we describe the procedure used for fitting this property (by 140 optimizing the parameters: β j , s d , γ , B j , and P C j ) to an ILI profile.
To allow for different models for the force of infection/contact rate, we write this term 142
in the most general way as a product of a basic force of infection, R 0 j , multiplied by 143 three time dependent terms:
The first time dependent term, F 1 (t), allows for a dependence of the transmission rate 145 on specific-humidity, the second (F 2 (t)) on the school vacation schedule, and the third 146 (F 3 (t)) allows the user to model an arbitrary behavior modification that can drive the 147 transmission rate up or down for a limited period of time. For the purpose of the CDC 148 challenge we only considered models involving either F 1 (t), F 2 (t), both, or none (i.e., 149 the contact rate does not depend on time), and the functional form of these terms is 150 discussed in sections S1 Text and S2 Text of the Supporting Information. curve was also fitted directly (without any regional information) using all the models 164 and priors, but these direct results were only used at the end of the season when 165 estimating the performance of each of our procedures. 166 In the coupled scenario, the MCMC procedure uses Eqs. (2) (3) (4) along with Eq. (10) to 167 simultaneously generate candidate profiles for the coupled ten HHS regions. The 168 log-likelihood of the ten regional profiles is calculated and combined with the proper 169 relative weights to generate a national log-likelihood which is minimized. It is important 170 to note that in the coupled scenario we only optimize the national log-likelihood, and 171 not the individual region-level likelihoods, but the parameters we optimize are still 172 mostly region specific (only s d and γ are not). We also tried fitting the coupled model 173 to the regional log-likelihoods, however the results of the fits were not as accurate as the 174 ones obtained when the national likelihood is optimized (see Discussion). In the previous section we described a traditional MCMC procedure which uses a log 191 uniform distribution for the parameters, which we term an uninformative prior (UP).
Early in the flu season, before the ILI curve takes off, this fitting can result in peak 193 intensities that are significantly larger/lower than expected (based on historic values) 194 and/or peak weeks that are inconsistent with past values. One way to constrain the 195 predictions, which has been used by others [28, 29] , is to use an informed prior (IP).
To generate informative priors, we used each of the models supported by DICE to fit all 197 previous seasons (starting from 2004) at both the national and regional levels. we also use a heated informed prior, where the Gaussian temperature is increased by an 218 order of magnitude (which is equivalent to increasing the variance by a factor of ten). In 219
the Results section we refer to the fitting procedures that use a prior as IP and HIP for 220 informed prior and heated informed prior, respectively. Informed priors were used only 221 with the uncoupled SIR Eqs. In a future study we plan to explore how they would 222 extend to the coupled MCMC procedure.
Using discounted historical data for not-yet-seen future time points 224 In addition to informative priors, we also used data augmentation to make maximum 225 use of prior data within a mechanistic framework. For each week during the challenge, 226 our data augmentation was a form of extrapolation in which future unobserved time 227 points were assumed to take either a historical average or values equal to those in the 228 most similar prior season. However, these historically augmented time points were not 229 counted within the likelihood with the same weight as actual observations. The
weighting was equal to the value of the Pearson correlation between the observed data 231 in the current year and the historical data for the same period from the year used for 232 augmentation. We shifted from the historic data to the most similar data at epidemic 233 week 6 (EW06) when we subjectively determined that the current season is very 234 different from the historic average. The augmented data was also y-shifted so that it used for both the coupled and uncoupled fits and also using a heated augmented 237 procedure (where the log-likelihood is again heated by a factor of ten During each of the CDC weeks DICE was used to fit both the regional and the national 242 most recent incidence data using the combinations of coupling, priors and models 
This total of 32 model-runs were used to make forecasts of incidence at both the 252 national and regional levels. For each region, we simulate three MCMC chains each with 253 10 7 steps and a burn time of 2 × 10 6 steps. The smallest effective sample size that we 254 report for any parameter was greater than 100. After sampling from the individual 255 posterior densities of each region, we calculated our national forecast as the weighted 256 sum of the regional profiles with the weights given by the relative populations of the 257 regions. The national curve was also fitted directly (without any regional information) 258 using all the models and priors, but these direct results were only used at the end of the 259 season when estimating the performance of each of our procedures.
Early in the season we were experimenting with the coupled procedure and we began to 261 use it as described in the manuscript with the DA and HDA priors only on EW 50 and 262
with the UP prior only on EW 9. Hence, some of the coupled results reported in this 263 section were not available in real-time and were generated at the end of the season (but 264 using only the %ILI data that was available in real-time at each forecast week.)
Each week a single forecast was selected from these results for each of the ten HHS 266 regions and the national. At the regional level we selected a single forecast from one of 267 the (32) uncoupled or coupled procedures enumerated in the previous paragraph. We given a forecast with a set of probabilities for p, with p i being the probability for an 285 observed outcome p i , the logarithmic score is:
For onset and peak week the score is calculated using the probability assigned to the 287 correct bin plus those of the preceding and proceeding bins (the bin size is one epidemic 288
week). For peak intensity and the 1 − 4 week forward forecast, the score is calculated 289 using the probability assigned to the correct bin plus those of the five preceding and 290 proceeding bins (the bin size is 0.1%).
Models selected for forecasts 293 We selected different FOI variants during different weeks. At the regional level, 294 although we selected the most flexible humidity and school vacation assumptions (HV) 295 more often (47.9% 134/280) than the alternatives (Fig 1) , we did select humidity-only 296 For assumptions about coupling, once the coupled procedure was available, it was often 322 selected for both the national profile and most regions (1, right panel), with the 323 exception of regions 1 and 8. We found that the coupled procedure used regions 8 (and 324 to a lesser extent 1) as a way to reduce the error to the national fit, at the cost of 325 producing poor fits to these regions, hence their coupled results were rarely selected for 326 submission. The aggregate option for the national selection was only selected at EWs 5 327 and 6, the weeks prior to the peak and the peak week itself. For these two weeks our 
Although our forecasts gave potentially useful information over and above the NULL 372 model for the timing of the peak week (Fig 3) and for the amplitude of peak intensity, 373 the peak week of EW06 was the same as the historical mean. Between EW50 (eight 374 weeks before the season peaks) and EW04 (two weeks before the season peak) our 375 forecast correctly predicted to within ±1 week of the observed peak week (EW06). One 376
week before the season peaks, and at the peak week (EW05 and EW06), our model 377 forecast has an error of two weeks.
. CC-BY 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint Forecasts based on the mechanistic model performed better than the historic NULL 379 model for the peak intensity (Fig 3 A /B/C/D). Two weeks before the peak week (and 380 three weeks early in the season) we started predicting the correct peak intensity of 5.1% 381 (to within ±0.5%). The mean and median historic values are significantly lower (4.4% 382 and 4.1% respectively) and outside the ±0.5% range. Our apparent forecast 383 performance for intensity appears to drop off at the end of the season. However, this is 384 an artifact of the forecasting work flow. Once the peak had clearly passed, the final 385 model was selected for reasons other than the peak intensity and the already-observed 386 peak intensity was submitted.
Selected forecasts based on the mechanistic model did not accurately predict onset. are arranged based on their CDC score (averaged across all weeks, numbers on the right 428 y-axis) from best (top) to worse (bottom). Coupled models were more accurate than The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint average model.
For predicting ILI incidence for the 2016-17 season, which followed similar trend to the 439 historical average, coupled models that used data augmentation were more accurate 440 than coupled models that did not use data augmentation. However, on average for 441 historical seasons, coupled models that did not use augmented data were more accurate 442 than those that did. Also, on average for historical seasons, coupled models that 443 included humidity were more accurate than those that did not (see dark banding in 444 upper portion of charts on the right hand side of Fig 4) . The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint We were able to compare our performance over the course of the season to the 481 performance of the other teams using the public website that supports the challenge 482 (www.cdc.gov/flusight). Averaged across all weeks of forecast and all forecast targets, 483 we were ranked 13 out of 29 teams. For 1-, 2-, 3-and 4-weeks ahead forecasts we were 484 ranked 6, 11, 9 and 16 respectively; again, out of 29 teams. We were ranked 14 for the 485 timing of onset, 5 for the timing of the peak and 14 for intensity of the peak. Probing 486 beyond the overall rankings, our performance was similar to the other better-performing 487 teams in the challenge. Also, our performance improved substantially as measured by 488 both in absolute terms and relative to other teams across the season (Figs S6 and S7). 489
In this study, we have described our participation in a prospective forecasting challenge. 491
Although we drew on results from a large set of mechanistic models, our single forecast 492 for each metric was made after choosing between available model results for that metric 493 in that week and was therefore somewhat subjective. We performed poorly at the start 494 PLOS 20/43 . CC-BY 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint of the competition when our mechanistic models consistently over-estimated incidence. 495
However, during the middle phase of the season, our models produced less biased 496 estimates and consistently outperformed non-mechanistic models based on the average 497 of historical data. A robust testing of model variants using historical data suggests that 498 spatially coupled models are systematically better than historical NULL models during 499 the middle of the season and are not significantly worse even at the start of the season. 500 We evaluated a simple ensemble and showed that the subjective model choice was 501 better. However, the ranking of individual models suggests that an ensemble of coupled 502 models may outperform our subjective choice. We are considering exactly this 503 experiment for the upcoming season.
This study is slightly different from some prior studies of influenza forecasting [30] in 505 that it describes and assesses a subjective choice between multiple mechanistic models 506 as the basis of a prospective forecast, rather than describing the performance of a single 507 model or single ensemble of models used for an entirely objective forecast. Although 508 this could be viewed as a limitation of our work, because individual subjective decisions 509 cannot be reproduced, we suggest that the explicit description of a partially subjective 510 process is a strength. In weather forecasting, there is a long history of evaluating the 511 accuracy of entirely objective forecasts versus partially subjective forecasts [31, 32] .
Broadly, for each different forecast target and each forecast lead-time, there has been a 513 gradual progression over time such that objective forecasts become more accurate than 514 subjective forecasts. We note also that although we describe the subjective process as it 515 was conducted, we also provide a thorough retrospective assessment of the predictive 516 performance of each model variant. 517 We may refine our ensemble approach for future iterations of the competition. It seems 518 clear that the coupled models produce more accurate forecasts than the uncoupled 519 models for most targets, so we would consider an ensemble only of the coupled model 520 variants. We will also consider weighted ensembles of models and attempt to find 521 optimal weights by studying all prior years. Also, data were often updated after being 522 reported and we did not include an explicit reporting model in our inferential framework 523 (also sometimes referred to as a backfill model). Rather, we used knowledge of past The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint adjustments to data during our discussions and eventual subjective choice of models. 525 We aim to include a formal reporting model in future versions of our framework. when evaluated using the historical data. This prospective study supports recent 530 retrospective results suggesting that influenza forecasts can be more accurate if they 531 explicitly represent spatial structure [33, 34] . Given that the model structure we used to 532 represent space was relatively coarse [24] , further work is warranted to test how forecast 533 accuracy at finer spatial scales can be improved by models that include iteratively finer 534 spatial resolution.
In submitting forecasts based on uninformed mechanistic priors using an uncoupled 536 model at the start of the season, we failed to learn lessons that have been present in the 537 influenza forecasting literature for some time [30] . Historical variance is low during the 538 start of the season and the growth pattern is not exponential. Therefore, it would be 539 reasonable to forecast early exponential growth only in the most exceptional of not-yet-seen time points, are likely to perform better. Also, forecasting competitions 543 may want to weight performance differentially across time, with greater weight given to 544 forecasts during periods where there is a higher variance in incidence.
Models that included humidity forcing performed better on average in our analysis of all 546 historical data than equivalent models that did not include those terms, especially for 547 the forecasting of ILI 1-to 4-weeks ahead [35] . However, we did not see similar support 548 for the inclusion of school vacation terms improving accuracy, which has been suggested 549 in a retrospective forecasting study at smaller spatial scales (by this group) [36] . The The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/309021 doi: bioRxiv preprint actual epidemiological week so that it was consistent with our presentation of accuracy 555 of other forecast targets. However, it may be more appropriate in some circumstances to 556 present accuracy of targets associated with the peak relative to the eventual peak [11] . 557 We found the experience of participating in a prospective forecasting challenge to be 558 different to that of a retrospective modeling study. The feedback in model accuracy was 559 much faster and the need for statistically robust measures of model likelihood or q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q EW44 and relate the local SH, q j (t), to the reproduction number as: The second term in Eq. 11 allows the transmission rate to depend on the weekly school 650 vacation schedule (p j (t)) and we implement is as:
DICE fits the effect of school closure by optimizing the parameter α, which is in the 
